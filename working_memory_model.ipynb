{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSY1401_Group5_Project4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "pEKK7l6V_kGi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### PSY1401 - Group 5 - Project 4\n",
        "\n",
        "Implementing the Working Memory Model of Botvinick & Watanabe (2007).\n",
        "\n",
        "Implemented by Peter Jankowski, Philippe NoÃ«l, and Lizzy Schick"
      ]
    },
    {
      "metadata": {
        "id": "rFQgSmwX_nq0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Task\n",
        "\n",
        "1. Implement the working memory model of Botvinick & Watanabe (2007), and show that it can reproduce positional accuracy and transposition gradient effects."
      ]
    },
    {
      "metadata": {
        "id": "Zf2Mskfl_qdh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm # colormap\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f-kb6mvD8YLI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will first implement the working memory model, simplifying it a little so to make for a more informative and simpler modeling."
      ]
    },
    {
      "metadata": {
        "id": "SZ0BBqB36jpI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 1.1 Implementing the working memory model"
      ]
    },
    {
      "metadata": {
        "id": "ixBhrgunv0mh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We first define some of the properties and parameters of the model."
      ]
    },
    {
      "metadata": {
        "id": "umL2A5kJv5sH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_item = 6 # number of items\n",
        "rank = 9 # Gershman said this is for granularity\n",
        "n_internal = n_item * rank # number of hidden layer nodes\n",
        "n_output = 720 # 720 possible target lists (6!)\n",
        "\n",
        "# set parameters !!!!!\n",
        "delta = 0        # degree of dissimilarity between item representations [0,1]\n",
        "alpha = 0.001    # learning rate, fixed in paper\n",
        "duration = 2500  # fixed in paper\n",
        "v = 0            # no noise during training\n",
        "delta_c = 0      # separation of confusable items\n",
        "delta_n = 0      # separation of non-confusable items\n",
        "delta_nc = 0     # separation of non-confusable and confusable items\n",
        "sigma = 0.5      # parameter set in paper"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IbICLNBMwxz9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We then define our matrices, which are our abstraction of the inwinded neural network from the paper."
      ]
    },
    {
      "metadata": {
        "id": "BeRIfagcyKwi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rho = np.zeros(rank)          # rank of maximal activation\n",
        "psi = np.zeros(n_item)          # optimal stimulus for item\n",
        "\n",
        "t = np.zeros(n_output)             # target values for output units, 0 or 1\n",
        "\n",
        "# 54 hidden layer nodes, 720 output, so we need 54 * 720 weights to combine\n",
        "# everything in the output layer\n",
        "h = np.zeros(n_internal)             # activations of internal units\n",
        "output = np.zeros((720, n_item)) # 720 possible lists (6!), 6 elements in each"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dF1BpbH-xCY0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Helper functions from various steps of the model."
      ]
    },
    {
      "metadata": {
        "id": "KNWBMt4txAXH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# unit activation function for item s\n",
        "def unit_activation(s):\n",
        "  if s==psi[s]:\n",
        "    return 1\n",
        "  else:\n",
        "    return 1-delta\n",
        "  \n",
        "# rank unit activation function for rank item r with preferred rank rho\n",
        "def rank_activation(r):\n",
        "  return np.exp(-1*((np.log(r)-np.log(rho[r]))**2)/(2*sigma**2))\n",
        "\n",
        "# change in internal unit activation for unit receiving input from rank r rho and item s\n",
        "def delta_internal_activation(r,s): \n",
        "  return rank_activation(r)*unit_activation(s)\n",
        "\n",
        "# output unit activation function for unit i\n",
        "def output_activation(i):\n",
        "  return (np.exp(a[i]))/(np.sum(np.exp(a)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q_S7q6tzyEi2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Excellent, so now we have defined all we need for our model. In the model, the hidden layer model is generated from the 6 items and 9 ranks by the delta_internal_activation formula with some prior initialization. Since there is no clear memory learning there, but instead simply a representation of the confusability of items, we skip this step altogether and hardcode the three potential cases of hidden layers in the form of heatmaps to facilitate the learning that goes on between the hidden layer and output layer."
      ]
    },
    {
      "metadata": {
        "id": "pHoY_I_76vnJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_min, x_max = 0, 9\n",
        "y_min, y_max = 0, 6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iJTj5bc1xLrH",
        "colab_type": "code",
        "outputId": "43da79f6-3dd5-40ac-fcca-ad84a7d52100",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 847
        }
      },
      "cell_type": "code",
      "source": [
        "# dissimilar items hidden layer (delta = 0.6)\n",
        "activation_patterns4a = [[1.4, 1.6, 1.7, 1.9, 1.6, 1.5,  1.3,  1.1, 0.5],\n",
        "                         [0.5, 2,   2.2, 2,   1.9, 1.45, 1.25, 1.05, 0.5],\n",
        "                         [0.7, 1.9, 2.3, 2.25, 2, 1.9, 1.45, 1.25, 1.05],\n",
        "                         [0.5, 1.6, 2.25, 2.3, 2.15, 1.95, 1.85, 1.4, 1.2],\n",
        "                         [0.3, 1.55, 2.2, 2.3, 2.2, 2, 1.65, 1.6, 1.35],\n",
        "                         [0.1, 1.5, 2, 2.2, 2.2, 2, 1.7, 1.6, 1.4]]\n",
        "\n",
        "# similar items hidden layer (delta = 0.4)\n",
        "activation_patterns4b = [[1.35, 2.5, 2.8, 2.8, 2.6, 2.55, 1.9, 1.7, 1.55],\n",
        "                         [1.4, 2.55, 2.9, 2.9, 2.7, 2.6, 1.8, 1.65, 1.55],\n",
        "                         [1.3, 2.5, 3, 3, 2.8, 2.7, 1.9, 1.7, 1.6],\n",
        "                         [1.2, 2.4, 3, 3, 2.9, 2.7, 2, 1.9, 1.7],\n",
        "                         [1.1, 2.55, 2.9, 3, 2.95, 2.75, 2.05, 1.95, 1.75],\n",
        "                         [1.1, 2.5, 2.85, 3, 2.9, 2.8, 2.1, 1.9, 1.75]]\n",
        "\n",
        "# dissimilar and similar items hidden layer (delta = 0.65)\n",
        "activation_patterns4c = [[1.5, 2.4, 2.6, 2.5, 2.4, 2.2, 1.75, 1.5, 1.2],\n",
        "                         [0.75, 1.75, 2, 1.75, 1.5, 1.3, 1, 0.75, 0.5],\n",
        "                         [1.2, 2.4, 3, 3, 2.6, 2.4, 1.75, 1.5, 1.2],\n",
        "                         [0.5, 1.5, 2, 1.85, 2, 1.75, 1.5, 1, 0.75],\n",
        "                         [1.2, 2.3, 2.8, 3, 2.9, 2.4, 2, 1.6, 1.35],\n",
        "                         [0.5, 1.3, 1.8, 1.9, 1.95, 1.8, 1.7, 1.5, 1.2]]\n",
        "\n",
        "# plotting\n",
        "plt.subplots(3,1,figsize=(8,14))\n",
        "\n",
        "# plotting dissimilar\n",
        "plt.subplot(311)\n",
        "plt.imshow(activation_patterns4a, extent=(x_min, x_max, y_max, y_min), interpolation='nearest', cmap=cm.hot)\n",
        "\n",
        "# plotting similar\n",
        "plt.subplot(312)\n",
        "plt.imshow(activation_patterns4b, extent=(x_min, x_max, y_max, y_min), interpolation='nearest', cmap=cm.hot)\n",
        "\n",
        "# plotting half and half\n",
        "plt.subplot(313)\n",
        "plt.imshow(activation_patterns4c, extent=(x_min, x_max, y_max, y_min), interpolation='nearest', cmap=cm.hot)\n",
        "\n",
        "# axes\n",
        "cax = plt.axes([0.95, 0.1, 0.1, 0.8])\n",
        "plt.colorbar(cax=cax)\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhAAAAM+CAYAAABL0Th7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAGAwBQQAMJgCAgAY7P8HRY7WWy98U+4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x1008 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "bjWrFrl51dgv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Alright cool, so we have our hidden layers for the three situations we want to model. Now, we want to train our model. So, we will initialize our weights at all 0 as per the paper, and train the weight vector here by mimicking a neural network's linear combination with the update rule of the paper. We first do it for the dissimilar items situation, after which we will do for the two other cases."
      ]
    },
    {
      "metadata": {
        "id": "GvLuoM_jF48o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "But, before we do that, we must define our labels to train our loss. We do so here with a dictionary and an arbitrary mapping. This will work as long as our mapping is consistent in our probabilities summing (I think?)"
      ]
    },
    {
      "metadata": {
        "id": "0qEOUUWD7I_S",
        "colab_type": "code",
        "outputId": "6ddd9455-040c-41de-9164-b5f6655041e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# compute permutations of a string\n",
        "def permute_string(s):\n",
        "    if len(s) == 0:\n",
        "        return ['']\n",
        "    prev_list = permute_string(s[1:len(s)])\n",
        "    next_list = []\n",
        "    for i in range(0,len(prev_list)):\n",
        "        for j in range(0,len(s)):\n",
        "            new_str = prev_list[i][0:j]+s[0]+prev_list[i][j:len(s)-1]\n",
        "            if new_str not in next_list:\n",
        "                next_list.append(new_str)\n",
        "    return next_list\n",
        "\n",
        "# compute for our sequence of items\n",
        "permutations = permute_string('123456')\n",
        "print(permutations)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['123456', '213456', '231456', '234156', '234516', '234561', '132456', '312456', '321456', '324156', '324516', '324561', '134256', '314256', '341256', '342156', '342516', '342561', '134526', '314526', '341526', '345126', '345216', '345261', '134562', '314562', '341562', '345162', '345612', '345621', '124356', '214356', '241356', '243156', '243516', '243561', '142356', '412356', '421356', '423156', '423516', '423561', '143256', '413256', '431256', '432156', '432516', '432561', '143526', '413526', '431526', '435126', '435216', '435261', '143562', '413562', '431562', '435162', '435612', '435621', '124536', '214536', '241536', '245136', '245316', '245361', '142536', '412536', '421536', '425136', '425316', '425361', '145236', '415236', '451236', '452136', '452316', '452361', '145326', '415326', '451326', '453126', '453216', '453261', '145362', '415362', '451362', '453162', '453612', '453621', '124563', '214563', '241563', '245163', '245613', '245631', '142563', '412563', '421563', '425163', '425613', '425631', '145263', '415263', '451263', '452163', '452613', '452631', '145623', '415623', '451623', '456123', '456213', '456231', '145632', '415632', '451632', '456132', '456312', '456321', '123546', '213546', '231546', '235146', '235416', '235461', '132546', '312546', '321546', '325146', '325416', '325461', '135246', '315246', '351246', '352146', '352416', '352461', '135426', '315426', '351426', '354126', '354216', '354261', '135462', '315462', '351462', '354162', '354612', '354621', '125346', '215346', '251346', '253146', '253416', '253461', '152346', '512346', '521346', '523146', '523416', '523461', '153246', '513246', '531246', '532146', '532416', '532461', '153426', '513426', '531426', '534126', '534216', '534261', '153462', '513462', '531462', '534162', '534612', '534621', '125436', '215436', '251436', '254136', '254316', '254361', '152436', '512436', '521436', '524136', '524316', '524361', '154236', '514236', '541236', '542136', '542316', '542361', '154326', '514326', '541326', '543126', '543216', '543261', '154362', '514362', '541362', '543162', '543612', '543621', '125463', '215463', '251463', '254163', '254613', '254631', '152463', '512463', '521463', '524163', '524613', '524631', '154263', '514263', '541263', '542163', '542613', '542631', '154623', '514623', '541623', '546123', '546213', '546231', '154632', '514632', '541632', '546132', '546312', '546321', '123564', '213564', '231564', '235164', '235614', '235641', '132564', '312564', '321564', '325164', '325614', '325641', '135264', '315264', '351264', '352164', '352614', '352641', '135624', '315624', '351624', '356124', '356214', '356241', '135642', '315642', '351642', '356142', '356412', '356421', '125364', '215364', '251364', '253164', '253614', '253641', '152364', '512364', '521364', '523164', '523614', '523641', '153264', '513264', '531264', '532164', '532614', '532641', '153624', '513624', '531624', '536124', '536214', '536241', '153642', '513642', '531642', '536142', '536412', '536421', '125634', '215634', '251634', '256134', '256314', '256341', '152634', '512634', '521634', '526134', '526314', '526341', '156234', '516234', '561234', '562134', '562314', '562341', '156324', '516324', '561324', '563124', '563214', '563241', '156342', '516342', '561342', '563142', '563412', '563421', '125643', '215643', '251643', '256143', '256413', '256431', '152643', '512643', '521643', '526143', '526413', '526431', '156243', '516243', '561243', '562143', '562413', '562431', '156423', '516423', '561423', '564123', '564213', '564231', '156432', '516432', '561432', '564132', '564312', '564321', '123465', '213465', '231465', '234165', '234615', '234651', '132465', '312465', '321465', '324165', '324615', '324651', '134265', '314265', '341265', '342165', '342615', '342651', '134625', '314625', '341625', '346125', '346215', '346251', '134652', '314652', '341652', '346152', '346512', '346521', '124365', '214365', '241365', '243165', '243615', '243651', '142365', '412365', '421365', '423165', '423615', '423651', '143265', '413265', '431265', '432165', '432615', '432651', '143625', '413625', '431625', '436125', '436215', '436251', '143652', '413652', '431652', '436152', '436512', '436521', '124635', '214635', '241635', '246135', '246315', '246351', '142635', '412635', '421635', '426135', '426315', '426351', '146235', '416235', '461235', '462135', '462315', '462351', '146325', '416325', '461325', '463125', '463215', '463251', '146352', '416352', '461352', '463152', '463512', '463521', '124653', '214653', '241653', '246153', '246513', '246531', '142653', '412653', '421653', '426153', '426513', '426531', '146253', '416253', '461253', '462153', '462513', '462531', '146523', '416523', '461523', '465123', '465213', '465231', '146532', '416532', '461532', '465132', '465312', '465321', '123645', '213645', '231645', '236145', '236415', '236451', '132645', '312645', '321645', '326145', '326415', '326451', '136245', '316245', '361245', '362145', '362415', '362451', '136425', '316425', '361425', '364125', '364215', '364251', '136452', '316452', '361452', '364152', '364512', '364521', '126345', '216345', '261345', '263145', '263415', '263451', '162345', '612345', '621345', '623145', '623415', '623451', '163245', '613245', '631245', '632145', '632415', '632451', '163425', '613425', '631425', '634125', '634215', '634251', '163452', '613452', '631452', '634152', '634512', '634521', '126435', '216435', '261435', '264135', '264315', '264351', '162435', '612435', '621435', '624135', '624315', '624351', '164235', '614235', '641235', '642135', '642315', '642351', '164325', '614325', '641325', '643125', '643215', '643251', '164352', '614352', '641352', '643152', '643512', '643521', '126453', '216453', '261453', '264153', '264513', '264531', '162453', '612453', '621453', '624153', '624513', '624531', '164253', '614253', '641253', '642153', '642513', '642531', '164523', '614523', '641523', '645123', '645213', '645231', '164532', '614532', '641532', '645132', '645312', '645321', '123654', '213654', '231654', '236154', '236514', '236541', '132654', '312654', '321654', '326154', '326514', '326541', '136254', '316254', '361254', '362154', '362514', '362541', '136524', '316524', '361524', '365124', '365214', '365241', '136542', '316542', '361542', '365142', '365412', '365421', '126354', '216354', '261354', '263154', '263514', '263541', '162354', '612354', '621354', '623154', '623514', '623541', '163254', '613254', '631254', '632154', '632514', '632541', '163524', '613524', '631524', '635124', '635214', '635241', '163542', '613542', '631542', '635142', '635412', '635421', '126534', '216534', '261534', '265134', '265314', '265341', '162534', '612534', '621534', '625134', '625314', '625341', '165234', '615234', '651234', '652134', '652314', '652341', '165324', '615324', '651324', '653124', '653214', '653241', '165342', '615342', '651342', '653142', '653412', '653421', '126543', '216543', '261543', '265143', '265413', '265431', '162543', '612543', '621543', '625143', '625413', '625431', '165243', '615243', '651243', '652143', '652413', '652431', '165423', '615423', '651423', '654123', '654213', '654231', '165432', '615432', '651432', '654132', '654312', '654321']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BerHnEn3GNdE",
        "colab_type": "code",
        "outputId": "3a33f882-5f2d-4f3b-fdb1-5f3ab612facf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 12257
        }
      },
      "cell_type": "code",
      "source": [
        "labels_mapping = dict() # were we store our labels\n",
        "i = 0 # index counter for mapping\n",
        "\n",
        "# dimensions of onehot vector\n",
        "onehot = np.zeros((720,1)) # 720 by 1\n",
        "\n",
        "# iterate over our permutations and store the mapping\n",
        "for permutation in permutations:\n",
        "  onehot[i] = 1 # set target index\n",
        "  i += 1 # increment for next\n",
        "  labels_mapping[permutation] = onehot # store\n",
        "  onehot = np.zeros((720,1)) # reset to no mapping for next\n",
        "\n",
        "# verify success\n",
        "print(labels_mapping['123456']) # looks right!"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qS_Lo-7bGNkp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Good, now we can train."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "R5diPrOS3EJU",
        "outputId": "9db07566-1b99-4540-90a8-cce792c26192",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# weights between internal and output units\n",
        "ws = np.zeros((n_internal, n_output))\n",
        "\n",
        "# train the weights - dissimilar items situation\n",
        "# reduce to 100 epochs to train faster\n",
        "for timestep in range(10): # train for 2500 epochs\n",
        "  # track progress\n",
        "  print(\"Epoch #\" + str(timestep))\n",
        "  \n",
        "  # for each epoch, we pass all of our 720 possible items\n",
        "  for permutation in labels_mapping:\n",
        "    \n",
        "    # our target value for this specific permutation\n",
        "    t_i = labels_mapping[permutation] # this is a 720x1 onehot array\n",
        "  \n",
        "    # theses will reset every timestep, which is what we want\n",
        "    a = np.zeros(n_output) # net input to units\n",
        "    o = np.zeros(n_output) # activations of the output units\n",
        "\n",
        "    for output_node_i in range(720): # train weights connection for each output node separatetly\n",
        "      # loop over our 54 hidden layer nodes\n",
        "      for i in range(n_item):\n",
        "        for j in range(rank):\n",
        "          # simply the linear combination of weights * input for this specific \n",
        "          # output node. This is our current predicted value for this output node\n",
        "          # at this specific timestep\n",
        "          a[output_node_i] += activation_patterns4a[i][j] * ws[i*j][output_node_i]\n",
        "\n",
        "    # now that we have our a_i, we can calculate our o_i\n",
        "    o = np.exp(a) / sum(np.exp(a)) # this is just applying the softmax function\n",
        "\n",
        "    # now that we have our o's, we can update our weights\n",
        "    # in the paper, a parameter is h_j, this parameter is simply the activation\n",
        "    # pattern at i,j in our activation patterns heatmap\n",
        "\n",
        "    # update our weight vector each weight separately\n",
        "    for i in range(n_item):\n",
        "      for j in range(rank):\n",
        "        h_j = activation_patterns4a[i][j] # indexing for our h_j\n",
        "        # loop over our 54 hidden layer nodes\n",
        "        for k in range(720):\n",
        "          # t_i is our target value, which is 1 if we have the right prediction\n",
        "          # and 0 otherwise\n",
        "          t_ik = t_i[k] # value of target for this specific guess\n",
        "\n",
        "          # update the weights!\n",
        "          ws[i*j][k] += alpha * (t_ik - o[k]) * h_j\n",
        "\n",
        "# training done!?"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch #0\n",
            "Epoch #1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RXU72GMoAGrl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In order to get the porportion of predicted times a number is found in its right position, we can just print the o vector and sum over our probabilities, which makes it a simple way to get the information to plot the working memory behavior without having to formerly run the model with actual inputs and outputs, since the law of large numbers will lead to a convergence to these probabilities anyways. Now that I think of it, I'm fairly sure this is probably how the researchers did it themselves. "
      ]
    },
    {
      "metadata": {
        "id": "Pjj2wbbK1dKc",
        "colab_type": "code",
        "outputId": "b4ab8205-68d2-4642-baf2-381edf8ad088",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2057
        }
      },
      "cell_type": "code",
      "source": [
        "# the final o vector we have stored is our updated predictions after 2500\n",
        "# timesteps, so we can just print it here to see how it looks\n",
        "print(o)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.00120487 0.0012053  0.00120573 0.00120617 0.0012066  0.00120703\n",
            " 0.00120746 0.00120789 0.00120833 0.00120876 0.00120919 0.00120962\n",
            " 0.00121006 0.00121049 0.00121093 0.00121136 0.00121179 0.00121223\n",
            " 0.00121266 0.0012131  0.00121354 0.00121397 0.00121441 0.00121484\n",
            " 0.00121528 0.00121572 0.00121615 0.00121659 0.00121703 0.00121747\n",
            " 0.00121791 0.00121834 0.00121878 0.00121922 0.00121966 0.0012201\n",
            " 0.00122054 0.00122098 0.00122142 0.00122186 0.0012223  0.00122274\n",
            " 0.00122318 0.00122363 0.00122407 0.00122451 0.00122495 0.00122539\n",
            " 0.00122584 0.00122628 0.00122672 0.00122717 0.00122761 0.00122806\n",
            " 0.0012285  0.00122895 0.00122939 0.00122984 0.00123028 0.00123073\n",
            " 0.00123117 0.00123162 0.00123207 0.00123252 0.00123296 0.00123341\n",
            " 0.00123386 0.00123431 0.00123475 0.0012352  0.00123565 0.0012361\n",
            " 0.00123655 0.001237   0.00123745 0.0012379  0.00123835 0.0012388\n",
            " 0.00123926 0.00123971 0.00124016 0.00124061 0.00124106 0.00124152\n",
            " 0.00124197 0.00124242 0.00124288 0.00124333 0.00124379 0.00124424\n",
            " 0.00124469 0.00124515 0.00124561 0.00124606 0.00124652 0.00124697\n",
            " 0.00124743 0.00124789 0.00124834 0.0012488  0.00124926 0.00124972\n",
            " 0.00125018 0.00125063 0.00125109 0.00125155 0.00125201 0.00125247\n",
            " 0.00125293 0.00125339 0.00125385 0.00125432 0.00125478 0.00125524\n",
            " 0.0012557  0.00125616 0.00125663 0.00125709 0.00125755 0.00125802\n",
            " 0.00125848 0.00125894 0.00125941 0.00125987 0.00126034 0.0012608\n",
            " 0.00126127 0.00126174 0.0012622  0.00126267 0.00126314 0.0012636\n",
            " 0.00126407 0.00126454 0.00126501 0.00126548 0.00126595 0.00126641\n",
            " 0.00126688 0.00126735 0.00126782 0.00126829 0.00126877 0.00126924\n",
            " 0.00126971 0.00127018 0.00127065 0.00127112 0.0012716  0.00127207\n",
            " 0.00127254 0.00127302 0.00127349 0.00127397 0.00127444 0.00127492\n",
            " 0.00127539 0.00127587 0.00127634 0.00127682 0.0012773  0.00127777\n",
            " 0.00127825 0.00127873 0.00127921 0.00127969 0.00128016 0.00128064\n",
            " 0.00128112 0.0012816  0.00128208 0.00128256 0.00128304 0.00128353\n",
            " 0.00128401 0.00128449 0.00128497 0.00128545 0.00128594 0.00128642\n",
            " 0.0012869  0.00128739 0.00128787 0.00128836 0.00128884 0.00128933\n",
            " 0.00128981 0.0012903  0.00129078 0.00129127 0.00129176 0.00129224\n",
            " 0.00129273 0.00129322 0.00129371 0.0012942  0.00129469 0.00129518\n",
            " 0.00129567 0.00129616 0.00129665 0.00129714 0.00129763 0.00129812\n",
            " 0.00129861 0.0012991  0.0012996  0.00130009 0.00130058 0.00130108\n",
            " 0.00130157 0.00130206 0.00130256 0.00130305 0.00130355 0.00130405\n",
            " 0.00130454 0.00130504 0.00130554 0.00130603 0.00130653 0.00130703\n",
            " 0.00130753 0.00130803 0.00130852 0.00130902 0.00130952 0.00131002\n",
            " 0.00131053 0.00131103 0.00131153 0.00131203 0.00131253 0.00131303\n",
            " 0.00131354 0.00131404 0.00131454 0.00131505 0.00131555 0.00131606\n",
            " 0.00131656 0.00131707 0.00131757 0.00131808 0.00131859 0.00131909\n",
            " 0.0013196  0.00132011 0.00132062 0.00132112 0.00132163 0.00132214\n",
            " 0.00132265 0.00132316 0.00132367 0.00132418 0.0013247  0.00132521\n",
            " 0.00132572 0.00132623 0.00132674 0.00132726 0.00132777 0.00132828\n",
            " 0.0013288  0.00132931 0.00132983 0.00133034 0.00133086 0.00133138\n",
            " 0.00133189 0.00133241 0.00133293 0.00133345 0.00133396 0.00133448\n",
            " 0.001335   0.00133552 0.00133604 0.00133656 0.00133708 0.0013376\n",
            " 0.00133812 0.00133865 0.00133917 0.00133969 0.00134021 0.00134074\n",
            " 0.00134126 0.00134179 0.00134231 0.00134284 0.00134336 0.00134389\n",
            " 0.00134441 0.00134494 0.00134547 0.001346   0.00134652 0.00134705\n",
            " 0.00134758 0.00134811 0.00134864 0.00134917 0.0013497  0.00135023\n",
            " 0.00135076 0.00135129 0.00135183 0.00135236 0.00135289 0.00135342\n",
            " 0.00135396 0.00135449 0.00135503 0.00135556 0.0013561  0.00135663\n",
            " 0.00135717 0.00135771 0.00135824 0.00135878 0.00135932 0.00135986\n",
            " 0.0013604  0.00136094 0.00136148 0.00136202 0.00136256 0.0013631\n",
            " 0.00136364 0.00136418 0.00136473 0.00136527 0.00136581 0.00136635\n",
            " 0.0013669  0.00136744 0.00136799 0.00136853 0.00136908 0.00136963\n",
            " 0.00137017 0.00137072 0.00137127 0.00137182 0.00137236 0.00137291\n",
            " 0.00137346 0.00137401 0.00137456 0.00137511 0.00137567 0.00137622\n",
            " 0.00137677 0.00137732 0.00137787 0.00137843 0.00137898 0.00137954\n",
            " 0.00138009 0.00138065 0.0013812  0.00138176 0.00138231 0.00138287\n",
            " 0.00138343 0.00138399 0.00138455 0.00138511 0.00138566 0.00138622\n",
            " 0.00138678 0.00138735 0.00138791 0.00138847 0.00138903 0.00138959\n",
            " 0.00139016 0.00139072 0.00139128 0.00139185 0.00139241 0.00139298\n",
            " 0.00139354 0.00139411 0.00139468 0.00139525 0.00139581 0.00139638\n",
            " 0.00139695 0.00139752 0.00139809 0.00139866 0.00139923 0.0013998\n",
            " 0.00140037 0.00140094 0.00140152 0.00140209 0.00140266 0.00140324\n",
            " 0.00140381 0.00140439 0.00140496 0.00140554 0.00140611 0.00140669\n",
            " 0.00140727 0.00140785 0.00140843 0.001409   0.00140958 0.00141016\n",
            " 0.00141074 0.00141132 0.00141191 0.00141249 0.00141307 0.00141365\n",
            " 0.00141424 0.00141482 0.0014154  0.00141599 0.00141657 0.00141716\n",
            " 0.00141775 0.00141833 0.00141892 0.00141951 0.0014201  0.00142069\n",
            " 0.00142127 0.00142186 0.00142245 0.00142305 0.00142364 0.00142423\n",
            " 0.00142482 0.00142541 0.00142601 0.0014266  0.0014272  0.00142779\n",
            " 0.00142839 0.00142898 0.00142958 0.00143018 0.00143077 0.00143137\n",
            " 0.00143197 0.00143257 0.00143317 0.00143377 0.00143437 0.00143497\n",
            " 0.00143557 0.00143617 0.00143678 0.00143738 0.00143798 0.00143859\n",
            " 0.00143919 0.0014398  0.0014404  0.00144101 0.00144162 0.00144222\n",
            " 0.00144283 0.00144344 0.00144405 0.00144466 0.00144527 0.00144588\n",
            " 0.00144649 0.0014471  0.00144772 0.00144833 0.00144894 0.00144956\n",
            " 0.00145017 0.00145079 0.0014514  0.00145202 0.00145263 0.00145325\n",
            " 0.00145387 0.00145449 0.00145511 0.00145572 0.00145634 0.00145697\n",
            " 0.00145759 0.00145821 0.00145883 0.00145945 0.00146008 0.0014607\n",
            " 0.00146132 0.00146195 0.00146257 0.0014632  0.00146383 0.00146445\n",
            " 0.00146508 0.00146571 0.00146634 0.00146697 0.0014676  0.00146823\n",
            " 0.00146886 0.00146949 0.00147012 0.00147076 0.00147139 0.00147203\n",
            " 0.00147266 0.00147329 0.00147393 0.00147457 0.0014752  0.00147584\n",
            " 0.00147648 0.00147712 0.00147776 0.0014784  0.00147904 0.00147968\n",
            " 0.00148032 0.00148096 0.0014816  0.00148225 0.00148289 0.00148354\n",
            " 0.00148418 0.00148483 0.00148547 0.00148612 0.00148677 0.00148742\n",
            " 0.00148806 0.00148871 0.00148936 0.00149001 0.00149066 0.00149132\n",
            " 0.00149197 0.00149262 0.00149327 0.00149393 0.00149458 0.00149524\n",
            " 0.00149589 0.00149655 0.00149721 0.00149787 0.00149852 0.00149918\n",
            " 0.00149984 0.0015005  0.00150116 0.00150182 0.00150249 0.00150315\n",
            " 0.00150381 0.00150448 0.00150514 0.0015058  0.00150647 0.00150714\n",
            " 0.0015078  0.00150847 0.00150914 0.00150981 0.00151048 0.00151115\n",
            " 0.00151182 0.00151249 0.00151316 0.00151383 0.00151451 0.00151518\n",
            " 0.00151586 0.00151653 0.00151721 0.00151788 0.00151856 0.00151924\n",
            " 0.00151992 0.00152059 0.00152127 0.00152195 0.00152264 0.00152332\n",
            " 0.001524   0.00152468 0.00152537 0.00152605 0.00152673 0.00152742\n",
            " 0.00152811 0.00152879 0.00152948 0.00153017 0.00153086 0.00153155\n",
            " 0.00153224 0.00153293 0.00153362 0.00153431 0.001535   0.0015357\n",
            " 0.00153639 0.00153708 0.00153778 0.00153848 0.00153917 0.00153987\n",
            " 0.00154057 0.00154127 0.00154197 0.00154267 0.00154337 0.00154407\n",
            " 0.00154477 0.00154547 0.00154618 0.00154688 0.00154758 0.00154829\n",
            " 0.001549   0.0015497  0.00155041 0.00155112 0.00155183 0.00155254\n",
            " 0.00155325 0.00155396 0.00155467 0.00155538 0.0015561  0.00155681\n",
            " 0.00155752 0.00155824 0.00155895 0.00155967 0.00156039 0.00156111\n",
            " 0.00156183 0.00156254 0.00156326 0.00156399 0.00156471 0.00156543\n",
            " 0.00156615 0.00156688 0.0015676  0.00156833 0.00156905 0.00156978\n",
            " 0.0015705  0.00157123 0.00157196 0.00157269 0.00157342 0.00157415\n",
            " 0.00157488 0.00157562 0.00157635 0.00157708 0.00157782 0.00157855\n",
            " 0.00157929 0.00158002 0.00158076 0.0015815  0.00158224 0.00158298\n",
            " 0.00158372 0.00158446 0.0015852  0.00158594 0.00158669 0.00158743\n",
            " 0.00158818 0.00158892 0.00158967 0.00159041 0.00159116 0.00159191\n",
            " 0.00159266 0.00159341 0.00159416 0.00159491 0.00159567 0.00159642\n",
            " 0.00159717 0.00159793 0.00159868 0.00159944 0.0016002  0.00160095\n",
            " 0.00160171 0.00160247 0.00160323 0.00160399 0.00160475 0.00160551\n",
            " 0.00160628 0.00160704 0.00160781 0.00160857 0.00160934 0.00161011\n",
            " 0.00161087 0.00161164 0.00161241 0.00161318 0.00161395 0.00099939]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4w98P4aFCI21",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In order to figure out which probabilities to sum with which, we need to map which index i,j is mapped with which item in which position. We built an arbitrary mapping earlier and used it for our training, so this is the mapping we will use here. We want to sum all indices where 1 is in position 1, all indices where 2 is in position 2, etc."
      ]
    },
    {
      "metadata": {
        "id": "_FYXoj_R1dND",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "624b566f-9cc2-405f-e906-e8e25f11cc36"
      },
      "cell_type": "code",
      "source": [
        "positions = np.array([0.0,0.0,0.0,0.0,0.0,0.0]) # where we will store\n",
        "\n",
        "i = 0 # counter to index in o vector\n",
        "for key, value in labels_mapping.items():\n",
        "  # check sequentially if each is in its right position  \n",
        "  if key[0] == '1':\n",
        "    positions[0] += o[i]\n",
        "  if key[1] == '2':\n",
        "    positions[1] += o[i]\n",
        "  if key[2] == '3':\n",
        "    positions[2] += o[i]\n",
        "  if key[3] == '4':\n",
        "    positions[3] += o[i]\n",
        "  if key[4] == '5':\n",
        "    positions[4] += o[i]\n",
        "  if key[5] == '6':\n",
        "    positions[5] += o[i]\n",
        "  # increment to the next probability\n",
        "  i += 1\n",
        "  \n",
        "# render to see how it looks\n",
        "print(positions)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.16659829 0.16629688 0.16618862 0.16815354 0.17095666 0.15004266]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gZDAz_cQxLtr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# also need to generalize the variable names and run it for the other activation patterns\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0scPPV8uxL0N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HQvQHbXy_q0J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8pY4wf0T_q8A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d5BBgV7FCYN5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Alright, our model works! All we need to do now is plot the graphs of the appropriate summed probabilities."
      ]
    },
    {
      "metadata": {
        "id": "GqFJgp44yEDl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 1.2 Reproduce positional accuracy (fig 5d)"
      ]
    },
    {
      "metadata": {
        "id": "7EbOaNcc-qAU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Em0BsQQYyLfu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 1.3 Reproduce transposition gradient effects (fig 5e)"
      ]
    },
    {
      "metadata": {
        "id": "6MkHuV5tye5A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f8o-OaJcebxp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}